{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d22115cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a382ec2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           textID                                               text  \\\n",
      "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
      "2      088c60f138                          my boss is bullying me...   \n",
      "3      9642c003ef                     what interview! leave me alone   \n",
      "4      358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
      "6      6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
      "...           ...                                                ...   \n",
      "27475  b78ec00df5                                     enjoy ur night   \n",
      "27476  4eac33d1c0   wish we could come see u on Denver  husband l...   \n",
      "27477  4f4c4fc327   I`ve wondered about rake to.  The client has ...   \n",
      "27478  f67aae2310   Yay good for both of you. Enjoy the break - y...   \n",
      "27479  ed167662a5                         But it was worth it  ****.   \n",
      "\n",
      "                    selected_text sentiment Time of Tweet Age of User  \\\n",
      "1                        Sooo SAD  negative          noon       21-30   \n",
      "2                     bullying me  negative         night       31-45   \n",
      "3                  leave me alone  negative       morning       46-60   \n",
      "4                   Sons of ****,  negative          noon       60-70   \n",
      "6                             fun  positive       morning        0-20   \n",
      "...                           ...       ...           ...         ...   \n",
      "27475                       enjoy  positive          noon       21-30   \n",
      "27476                      d lost  negative         night       31-45   \n",
      "27477               , don`t force  negative       morning       46-60   \n",
      "27478   Yay good for both of you.  positive          noon       60-70   \n",
      "27479  But it was worth it  ****.  positive         night      70-100   \n",
      "\n",
      "         Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
      "1        Albania           2877797          27400.0              105  \n",
      "2        Algeria          43851044        2381740.0               18  \n",
      "3        Andorra             77265            470.0              164  \n",
      "4         Angola          32866272        1246700.0               26  \n",
      "6      Argentina          45195774        2736690.0               17  \n",
      "...          ...               ...              ...              ...  \n",
      "27475    Germany          83783942         348560.0              240  \n",
      "27476      Ghana          31072940         227540.0              137  \n",
      "27477     Greece          10423054         128900.0               81  \n",
      "27478    Grenada            112523            340.0              331  \n",
      "27479  Guatemala          17915568         107160.0              167  \n",
      "\n",
      "[16363 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Replace the file path with the correct one\n",
    "file_path = r\"C:\\Users\\91702\\Downloads\\train.csv\"\n",
    "\n",
    "# Try reading the file with 'latin1' encoding\n",
    "\n",
    "df = pd.read_csv(file_path, encoding='latin1')\n",
    "# Further processing or modifications as needed\n",
    "df.drop(df[df['sentiment'] == \"neutral\"].index, inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9458afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          textID                                               text sentiment  \\\n",
      "1     96d74cb729   Shanghai is also really exciting (precisely -...  positive   \n",
      "2     eee518ae67  Recession hit Veronique Branquinho, she has to...  negative   \n",
      "3     01082688c6                                        happy bday!  positive   \n",
      "4     33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive   \n",
      "5     726e501993                    that`s great!! weee!! visitors!  positive   \n",
      "...          ...                                                ...       ...   \n",
      "3529  e5f0e6ef4b  its at 3 am, im very tired but i can`t sleep  ...  negative   \n",
      "3530  416863ce47  All alone in this old house again.  Thanks for...  positive   \n",
      "3531  6332da480c   I know what you mean. My little dog is sinkin...  negative   \n",
      "3532  df1baec676  _sutra what is your next youtube video gonna b...  positive   \n",
      "3533  469e15c5a8   http://twitpic.com/4woj2 - omgssh  ang cute n...  positive   \n",
      "\n",
      "     Time of Tweet Age of User              Country  Population -2020  \\\n",
      "1             noon       21-30              Albania         2877797.0   \n",
      "2            night       31-45              Algeria        43851044.0   \n",
      "3          morning       46-60              Andorra           77265.0   \n",
      "4             noon       60-70               Angola        32866272.0   \n",
      "5            night      70-100  Antigua and Barbuda           97929.0   \n",
      "...            ...         ...                  ...               ...   \n",
      "3529          noon       21-30            Nicaragua         6624554.0   \n",
      "3530         night       31-45                Niger        24206644.0   \n",
      "3531       morning       46-60              Nigeria       206139589.0   \n",
      "3532          noon       60-70          North Korea        25778816.0   \n",
      "3533         night      70-100      North Macedonia         2083374.0   \n",
      "\n",
      "      Land Area (Km²)  Density (P/Km²)  \n",
      "1             27400.0            105.0  \n",
      "2           2381740.0             18.0  \n",
      "3               470.0            164.0  \n",
      "4           1246700.0             26.0  \n",
      "5               440.0            223.0  \n",
      "...               ...              ...  \n",
      "3529         120340.0             55.0  \n",
      "3530        1266700.0             19.0  \n",
      "3531         910770.0            226.0  \n",
      "3532         120410.0            214.0  \n",
      "3533          25220.0             83.0  \n",
      "\n",
      "[2104 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Replace the file path with the correct one\n",
    "file_path1 = r\"C:\\Users\\91702\\Downloads\\test.csv\"\n",
    "\n",
    "# Try reading the file with 'latin1' encoding\n",
    "\n",
    "df1 = pd.read_csv(file_path1, encoding='latin1')\n",
    "# Further processing or modifications as needed\n",
    "df1.drop(df1[df1['sentiment'] == \"neutral\"].index, inplace=True)\n",
    "df1.dropna(inplace=True)\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1098890d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91702\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "# Instantiate tokenizer, stemmer, and stopwords\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "ps = PorterStemmer()\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Function for cleaning text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function for cleaning text with Lemmatization\n",
    "def getCleanedText(text):\n",
    "    # Convert to string (in case it's not already)\n",
    "    #text = str(text)\n",
    "    \n",
    "    # Remove digits\n",
    "    clean_text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    clean_text = re.sub(r'http\\S+|www.\\S+|mailto:\\S+', '', clean_text)\n",
    "    \n",
    "    clean_text = re.sub(r'#\\w+', '', text)\n",
    "\n",
    "    # Remove mentions\n",
    "    clean_text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    clean_text = re.sub(r'[^a-zA-Z\\s]', '', clean_text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    clean_text = ' '.join(clean_text.split())\n",
    "    \n",
    "    # Remove diacritics (accents)\n",
    "    clean_text = ''.join(c for c in unicodedata.normalize('NFD', clean_text) if unicodedata.category(c) != 'Mn')\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    clean_text = clean_text.lower()\n",
    "    \n",
    "    # Tokenize the cleaned text\n",
    "    tokens = tokenizer.tokenize(clean_text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    new_tokens = [token for token in tokens if token not in en_stopwords]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in new_tokens]\n",
    "    \n",
    "    # Join tokens back into a cleaned text\n",
    "    clean_text = \" \".join(lemmatized_tokens)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "973fa19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column of cleaned data\n",
    "df['new'] = df['text'].apply(getCleanedText)\n",
    "df1['new'] = df1['text'].apply(getCleanedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0993d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the values for x_train, y_train, x_test, y_test\n",
    "x_train = df['new']\n",
    "y_train = df['sentiment']\n",
    "x_test = df1['new']\n",
    "y_test = df1['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92504d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.02%\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=25000)  # You can adjust max_features as needed\n",
    "x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "x_test_tfidf = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "# Initialize SVC classifier\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(x_test_tfidf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
